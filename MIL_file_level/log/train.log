Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../../huggingface_models/microsoft/codebert-base/ and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
04/19/2024 15:18:30 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../data/py/train.jsonl', output_dir='./saved_models', language_type='py', eval_data_file='../data/py/valid.jsonl', test_data_file='../data/py/test.jsonl', model_name_or_path='../../huggingface_models/microsoft/codebert-base/', block_size=400, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, epoch=5, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, seed=123456, min_loss_delta=0.001, dropout_probability=0, device=device(type='cuda'), n_gpu=2, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=32, start_epoch=0, start_step=0)
04/19/2024 15:18:47 - INFO - __main__ -   *** Example ***
04/19/2024 15:18:47 - INFO - __main__ -   idx: 0
04/19/2024 15:18:47 - INFO - __main__ -   file_id: CWE-326/py/bad_4376_2
04/19/2024 15:18:47 - INFO - __main__ -   label: 1
04/19/2024 15:18:47 - INFO - __main__ -   input_tokens: ['<s>', '"""', 'Factory', '_functions', '_for', '_asymm', 'etric', '_cryptography', '."', '""', '_from', '_.', 'comp', 'at', '_import', '_*', '_from', '_.', 'rs', 'ake', 'y', '_import', '_RSA', 'Key', '_from', '_.', 'python', '_', 'rs', 'ake', 'y', '_import', '_Python', '_', 'R', 'SA', 'Key', '_from', '_t', 'l', 'sl', 'ite', '.', 'utils', '_import', '_crypt', 'om', 'ath', '_if', '_crypt', 'om', 'ath', '.', 'm', '2', 'crypt', 'o', 'Load', 'ed', ':', '_from', '_.', 'opens', 'sl', '_', 'rs', 'ake', 'y', '_import', '_Open', 'SSL', '_', 'R', 'SA', 'Key', '_if', '_crypt', 'om', 'ath', '.', 'py', 'crypt', 'o', 'Load', 'ed', ':', '_from', '_.', 'py', 'crypt', 'o', '_', 'rs', 'ake', 'y', '_import', '_Py', 'Crypt', 'o', '_', 'R', 'SA', 'Key', '_def', '_generate', 'R', 'SA', 'Key', '(', 'bits', ',', '_implementations', '=', '["', 'opens', 'sl', '",', '_"', 'python', '"]', '):', '_"""', 'Gener', 'ate', '_an', '_RSA', '_key', '_with', '_the', '_specified', '_bit', '_length', '.', '_:', 'type', '_bits', ':', '_int', '_:', 'param', '_bits', ':', '_Des', 'ired', '_bit', '_length', '_of', '_the', '_new', '_key', "'s", '_mod', 'ulus', '.', '_:', 'r', 'type', ':', '_~', 'tl', 'sl', 'ite', '.', 'utils', '.', 'rs', 'ake', 'y', '.', 'R', 'SA', 'Key', '_:', 'return', 's', ':', '_A', '_new', '_RSA', '_private', '_key', '.', '_"""', '_for', '_implementation', '_in', '_implementations', ':', '_if', '_implementation', '_==', '_"', 'opens', 'sl', '"', '_and', '_crypt', 'om', 'ath', '.', 'm', '2', 'crypt', 'o', 'Load', 'ed', ':', '_return', '_Open', 'SSL', '_', 'R', 'SA', 'Key', '.', 'gener', 'ate', '(', 'bits', ')', '_el', 'if', '_implementation', '_==', '_"', 'python', '":', '_return', '_Python', '_', 'R', 'SA', 'Key', '.', 'gener', 'ate', '(', 'bits', ')', '_raise', '_Value', 'Error', '("', 'No', '_acceptable', '_implementations', '")', '_def', '_parse', 'P', 'EM', 'Key', '(', 's', ',', '_private', '=', 'False', ',', '_public', '=', 'False', ',', '_password', 'Callback', '=', 'None', ',', '_implementations', '=', '["', 'opens', 'sl', '",', '_"', 'python', '"]', '):', '_"""', 'Par', 'se', '_a', '_P', 'EM', '-', 'format', '_key', '.', '_The', '_P', 'EM', '_format', '_is', '_used', '_by', '_Open', 'SSL', '_and', '_other', '_tools', '.', '_The', '_format', '_is', '_typically', '_used', '_to', '_store', '_both', '_the', '_public', '_and', '_private', '_components', '_of', '_a', '_key', '.', '_For', '_example', '::', '_-----', 'B', 'EGIN', '_RSA', '_PR', 'IV', 'ATE', '_KEY', '-----', '_MI', 'IC', 'X', 'Q', 'IB', 'AA', 'KB', 'g', 'Q', 'D', 'Y', 'sc', 'uo', 'M', 'z', 's', 'G', 'm', 'W', '0', 'p', 'AY', 's', 'my', 'H', 'lt', 'x', 'B', '2', 'T', 'd', 'w', 'HS', '0', 'd', 'Im', 'f', 'j', 'CM', 'fa', 'SD', 'k', 'f', 'L', 'd', 'Z', 'Y', '5', '+', '_d', 'OW', 'OR', 'V', 'ns', '9', 'et', 'W', 'nr', '194', 'm', 'S', 'GA', '1', 'F', '0', 'Pl', 's', '/', 'V', '</s>']
04/19/2024 15:18:47 - INFO - __main__ -   input_ids: 0 49849 47249 8047 13 34228 25286 45369 72 48149 31 479 11828 415 6595 1009 31 479 4926 5113 219 6595 36703 28152 31 479 49119 1215 4926 5113 219 6595 31886 1215 500 3603 28152 31 326 462 9996 1459 4 49320 6595 35867 1075 2681 114 35867 1075 2681 4 119 176 44675 139 47167 196 35 31 479 35800 9996 1215 4926 5113 219 6595 2117 46535 1215 500 3603 28152 114 35867 1075 2681 4 17163 44675 139 47167 196 35 31 479 17163 44675 139 1215 4926 5113 219 6595 19972 44623 139 1215 500 3603 28152 3816 5368 500 3603 28152 1640 32131 6 42993 5214 49329 35800 9996 1297 22 49119 42248 3256 49434 40025 877 41 36703 762 19 5 17966 828 5933 4 4832 12528 15239 35 6979 4832 46669 15239 35 4762 7651 828 5933 9 5 92 762 18 11134 35223 4 4832 338 12528 35 14434 34842 9996 1459 4 49320 4 4926 5113 219 4 500 3603 28152 4832 30921 29 35 83 92 36703 940 762 4 49434 13 5574 11 42993 35 114 5574 45994 22 35800 9996 113 8 35867 1075 2681 4 119 176 44675 139 47167 196 35 671 2117 46535 1215 500 3603 28152 4 20557 877 1640 32131 43 1615 1594 5574 45994 22 49119 7862 671 31886 1215 500 3603 28152 4 20557 877 1640 32131 43 1693 11714 30192 46469 3084 9796 42993 8070 3816 43756 510 5330 28152 1640 29 6 940 5214 46659 6 285 5214 46659 6 14844 49706 5214 29802 6 42993 5214 49329 35800 9996 1297 22 49119 42248 3256 49434 22011 1090 10 221 5330 12 34609 762 4 20 221 5330 7390 16 341 30 2117 46535 8 97 3270 4 20 7390 16 3700 341 7 1400 258 5 285 8 940 6411 9 10 762 4 286 1246 38304 45364 387 39764 36703 4729 6372 8625 23993 46343 10931 2371 1000 1864 8863 5596 36640 571 1864 495 975 3866 23613 448 329 29 534 119 771 288 642 2547 29 4783 725 7984 1178 387 176 565 417 605 6391 288 417 20470 506 267 18814 12010 6243 330 506 574 417 1301 975 245 2744 385 4581 3411 846 6852 466 594 771 37643 30548 119 104 4164 134 597 288 16213 29 73 846 2
04/19/2024 15:18:47 - INFO - __main__ -   *** Example ***
04/19/2024 15:18:47 - INFO - __main__ -   idx: 1
04/19/2024 15:18:47 - INFO - __main__ -   file_id: CWE-326/py/good_4376_2
04/19/2024 15:18:47 - INFO - __main__ -   label: 0
04/19/2024 15:18:47 - INFO - __main__ -   input_tokens: ['<s>', '"""', 'Factory', '_functions', '_for', '_asymm', 'etric', '_cryptography', '."', '""', '_from', '_.', 'comp', 'at', '_import', '_*', '_from', '_.', 'rs', 'ake', 'y', '_import', '_RSA', 'Key', '_from', '_.', 'python', '_', 'rs', 'ake', 'y', '_import', '_Python', '_', 'R', 'SA', 'Key', '_from', '_t', 'l', 'sl', 'ite', '.', 'utils', '_import', '_crypt', 'om', 'ath', '_if', '_crypt', 'om', 'ath', '.', 'm', '2', 'crypt', 'o', 'Load', 'ed', ':', '_from', '_.', 'opens', 'sl', '_', 'rs', 'ake', 'y', '_import', '_Open', 'SSL', '_', 'R', 'SA', 'Key', '_if', '_crypt', 'om', 'ath', '.', 'py', 'crypt', 'o', 'Load', 'ed', ':', '_from', '_.', 'py', 'crypt', 'o', '_', 'rs', 'ake', 'y', '_import', '_Py', 'Crypt', 'o', '_', 'R', 'SA', 'Key', '_def', '_generate', 'R', 'SA', 'Key', '(', 'bits', ',', '_implementations', '=', '["', 'opens', 'sl', '",', '_"', 'python', '"]', '):', '_"""', 'Gener', 'ate', '_an', '_RSA', '_key', '_with', '_the', '_specified', '_bit', '_length', '.', '_:', 'type', '_bits', ':', '_int', '_:', 'param', '_bits', ':', '_Des', 'ired', '_bit', '_length', '_of', '_the', '_new', '_key', "'s", '_mod', 'ulus', '.', '_:', 'r', 'type', ':', '_~', 'tl', 'sl', 'ite', '.', 'utils', '.', 'rs', 'ake', 'y', '.', 'R', 'SA', 'Key', '_:', 'return', 's', ':', '_A', '_new', '_RSA', '_private', '_key', '.', '_"""', '_for', '_implementation', '_in', '_implementations', ':', '_if', '_implementation', '_==', '_"', 'opens', 'sl', '"', '_and', '_crypt', 'om', 'ath', '.', 'm', '2', 'crypt', 'o', 'Load', 'ed', ':', '_return', '_Open', 'SSL', '_', 'R', 'SA', 'Key', '.', 'gener', 'ate', '(', 'bits', ')', '_el', 'if', '_implementation', '_==', '_"', 'python', '":', '_return', '_Python', '_', 'R', 'SA', 'Key', '.', 'gener', 'ate', '(', 'bits', ')', '_raise', '_Value', 'Error', '("', 'No', '_acceptable', '_implementations', '")', '_def', '_parse', 'P', 'EM', 'Key', '(', 's', ',', '_private', '=', 'False', ',', '_public', '=', 'False', ',', '_password', 'Callback', '=', 'None', ',', '_implementations', '=', '["', 'opens', 'sl', '",', '_"', 'python', '"]', '):', '_"""', 'Par', 'se', '_a', '_P', 'EM', '-', 'format', '_key', '.', '_The', '_P', 'EM', '_format', '_is', '_used', '_by', '_Open', 'SSL', '_and', '_other', '_tools', '.', '_The', '_format', '_is', '_typically', '_used', '_to', '_store', '_both', '_the', '_public', '_and', '_private', '_components', '_of', '_a', '_key', '.', '_For', '_example', '::', '_-----', 'B', 'EGIN', '_RSA', '_PR', 'IV', 'ATE', '_KEY', '-----', '_MI', 'IC', 'X', 'Q', 'IB', 'AA', 'KB', 'g', 'Q', 'D', 'Y', 'sc', 'uo', 'M', 'z', 's', 'G', 'm', 'W', '0', 'p', 'AY', 's', 'my', 'H', 'lt', 'x', 'B', '2', 'T', 'd', 'w', 'HS', '0', 'd', 'Im', 'f', 'j', 'CM', 'fa', 'SD', 'k', 'f', 'L', 'd', 'Z', 'Y', '5', '+', '_d', 'OW', 'OR', 'V', 'ns', '9', 'et', 'W', 'nr', '194', 'm', 'S', 'GA', '1', 'F', '0', 'Pl', 's', '/', 'V', '</s>']
04/19/2024 15:18:47 - INFO - __main__ -   input_ids: 0 49849 47249 8047 13 34228 25286 45369 72 48149 31 479 11828 415 6595 1009 31 479 4926 5113 219 6595 36703 28152 31 479 49119 1215 4926 5113 219 6595 31886 1215 500 3603 28152 31 326 462 9996 1459 4 49320 6595 35867 1075 2681 114 35867 1075 2681 4 119 176 44675 139 47167 196 35 31 479 35800 9996 1215 4926 5113 219 6595 2117 46535 1215 500 3603 28152 114 35867 1075 2681 4 17163 44675 139 47167 196 35 31 479 17163 44675 139 1215 4926 5113 219 6595 19972 44623 139 1215 500 3603 28152 3816 5368 500 3603 28152 1640 32131 6 42993 5214 49329 35800 9996 1297 22 49119 42248 3256 49434 40025 877 41 36703 762 19 5 17966 828 5933 4 4832 12528 15239 35 6979 4832 46669 15239 35 4762 7651 828 5933 9 5 92 762 18 11134 35223 4 4832 338 12528 35 14434 34842 9996 1459 4 49320 4 4926 5113 219 4 500 3603 28152 4832 30921 29 35 83 92 36703 940 762 4 49434 13 5574 11 42993 35 114 5574 45994 22 35800 9996 113 8 35867 1075 2681 4 119 176 44675 139 47167 196 35 671 2117 46535 1215 500 3603 28152 4 20557 877 1640 32131 43 1615 1594 5574 45994 22 49119 7862 671 31886 1215 500 3603 28152 4 20557 877 1640 32131 43 1693 11714 30192 46469 3084 9796 42993 8070 3816 43756 510 5330 28152 1640 29 6 940 5214 46659 6 285 5214 46659 6 14844 49706 5214 29802 6 42993 5214 49329 35800 9996 1297 22 49119 42248 3256 49434 22011 1090 10 221 5330 12 34609 762 4 20 221 5330 7390 16 341 30 2117 46535 8 97 3270 4 20 7390 16 3700 341 7 1400 258 5 285 8 940 6411 9 10 762 4 286 1246 38304 45364 387 39764 36703 4729 6372 8625 23993 46343 10931 2371 1000 1864 8863 5596 36640 571 1864 495 975 3866 23613 448 329 29 534 119 771 288 642 2547 29 4783 725 7984 1178 387 176 565 417 605 6391 288 417 20470 506 267 18814 12010 6243 330 506 574 417 1301 975 245 2744 385 4581 3411 846 6852 466 594 771 37643 30548 119 104 4164 134 597 288 16213 29 73 846 2
04/19/2024 15:18:50 - INFO - __main__ -   ***** Running training *****
04/19/2024 15:18:50 - INFO - __main__ -     Num examples = 806
04/19/2024 15:18:50 - INFO - __main__ -     Num Epochs = 5
04/19/2024 15:18:50 - INFO - __main__ -     Instantaneous batch size per GPU = 16
04/19/2024 15:18:50 - INFO - __main__ -     Total train batch size = 32
04/19/2024 15:18:50 - INFO - __main__ -     Gradient Accumulation steps = 1
04/19/2024 15:18:50 - INFO - __main__ -     Total optimization steps = 130
  0%|          | 0/26 [00:00<?, ?it/s]/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
epoch 0 step 1 loss 0.69069:   0%|          | 0/26 [00:03<?, ?it/s]epoch 0 step 1 loss 0.69069:   4%|▍         | 1/26 [00:03<01:35,  3.81s/it]epoch 0 step 2 loss 0.68951:   4%|▍         | 1/26 [00:04<01:35,  3.81s/it]epoch 0 step 2 loss 0.68951:   8%|▊         | 2/26 [00:04<00:43,  1.82s/it]epoch 0 step 3 loss 0.69357:   8%|▊         | 2/26 [00:04<00:43,  1.82s/it]epoch 0 step 3 loss 0.69357:  12%|█▏        | 3/26 [00:04<00:26,  1.15s/it]epoch 0 step 4 loss 0.69287:  12%|█▏        | 3/26 [00:04<00:26,  1.15s/it]epoch 0 step 4 loss 0.69287:  15%|█▌        | 4/26 [00:04<00:18,  1.20it/s]epoch 0 step 5 loss 0.69242:  15%|█▌        | 4/26 [00:05<00:18,  1.20it/s]epoch 0 step 5 loss 0.69242:  19%|█▉        | 5/26 [00:05<00:13,  1.53it/s]epoch 0 step 6 loss 0.69722:  19%|█▉        | 5/26 [00:05<00:13,  1.53it/s]epoch 0 step 6 loss 0.69722:  23%|██▎       | 6/26 [00:05<00:11,  1.81it/s]epoch 0 step 7 loss 0.69843:  23%|██▎       | 6/26 [00:05<00:11,  1.81it/s]epoch 0 step 7 loss 0.69843:  27%|██▋       | 7/26 [00:05<00:09,  2.07it/s]epoch 0 step 8 loss 0.70035:  27%|██▋       | 7/26 [00:06<00:09,  2.07it/s]epoch 0 step 8 loss 0.70035:  31%|███       | 8/26 [00:06<00:07,  2.28it/s]epoch 0 step 9 loss 0.69925:  31%|███       | 8/26 [00:06<00:07,  2.28it/s]epoch 0 step 9 loss 0.69925:  35%|███▍      | 9/26 [00:06<00:06,  2.45it/s]epoch 0 step 10 loss 0.69968:  35%|███▍      | 9/26 [00:06<00:06,  2.45it/s]epoch 0 step 10 loss 0.69968:  38%|███▊      | 10/26 [00:06<00:06,  2.58it/s]epoch 0 step 11 loss 0.70102:  38%|███▊      | 10/26 [00:07<00:06,  2.58it/s]epoch 0 step 11 loss 0.70102:  42%|████▏     | 11/26 [00:07<00:05,  2.67it/s]epoch 0 step 12 loss 0.70082:  42%|████▏     | 11/26 [00:07<00:05,  2.67it/s]epoch 0 step 12 loss 0.70082:  46%|████▌     | 12/26 [00:07<00:05,  2.75it/s]epoch 0 step 13 loss 0.70136:  46%|████▌     | 12/26 [00:08<00:05,  2.75it/s]epoch 0 step 13 loss 0.70136:  50%|█████     | 13/26 [00:08<00:04,  2.80it/s]epoch 0 step 14 loss 0.70092:  50%|█████     | 13/26 [00:08<00:04,  2.80it/s]epoch 0 step 14 loss 0.70092:  54%|█████▍    | 14/26 [00:08<00:04,  2.83it/s]epoch 0 step 15 loss 0.70143:  54%|█████▍    | 14/26 [00:08<00:04,  2.83it/s]epoch 0 step 15 loss 0.70143:  58%|█████▊    | 15/26 [00:08<00:03,  2.86it/s]epoch 0 step 16 loss 0.70097:  58%|█████▊    | 15/26 [00:09<00:03,  2.86it/s]epoch 0 step 16 loss 0.70097:  62%|██████▏   | 16/26 [00:09<00:03,  2.88it/s]epoch 0 step 17 loss 0.70047:  62%|██████▏   | 16/26 [00:09<00:03,  2.88it/s]epoch 0 step 17 loss 0.70047:  65%|██████▌   | 17/26 [00:09<00:03,  2.88it/s]epoch 0 step 18 loss 0.70131:  65%|██████▌   | 17/26 [00:09<00:03,  2.88it/s]epoch 0 step 18 loss 0.70131:  69%|██████▉   | 18/26 [00:09<00:02,  2.88it/s]epoch 0 step 19 loss 0.70131:  69%|██████▉   | 18/26 [00:10<00:02,  2.88it/s]epoch 0 step 19 loss 0.70131:  73%|███████▎  | 19/26 [00:10<00:02,  2.89it/s]epoch 0 step 20 loss 0.70134:  73%|███████▎  | 19/26 [00:10<00:02,  2.89it/s]epoch 0 step 20 loss 0.70134:  77%|███████▋  | 20/26 [00:10<00:02,  2.90it/s]epoch 0 step 21 loss 0.69954:  77%|███████▋  | 20/26 [00:10<00:02,  2.90it/s]epoch 0 step 21 loss 0.69954:  81%|████████  | 21/26 [00:10<00:01,  2.91it/s]epoch 0 step 22 loss 0.69969:  81%|████████  | 21/26 [00:11<00:01,  2.91it/s]epoch 0 step 22 loss 0.69969:  85%|████████▍ | 22/26 [00:11<00:01,  2.91it/s]epoch 0 step 23 loss 0.69938:  85%|████████▍ | 22/26 [00:11<00:01,  2.91it/s]epoch 0 step 23 loss 0.69938:  88%|████████▊ | 23/26 [00:11<00:01,  2.92it/s]epoch 0 step 24 loss 0.70017:  88%|████████▊ | 23/26 [00:11<00:01,  2.92it/s]epoch 0 step 24 loss 0.70017:  92%|█████████▏| 24/26 [00:11<00:00,  2.92it/s]epoch 0 step 25 loss 0.69977:  92%|█████████▏| 24/26 [00:12<00:00,  2.92it/s]epoch 0 step 25 loss 0.69977:  96%|█████████▌| 25/26 [00:12<00:00,  2.93it/s]epoch 0 step 26 loss 0.70064:  96%|█████████▌| 25/26 [00:12<00:00,  2.93it/s]04/19/2024 15:19:06 - INFO - __main__ -   ***** Running evaluation *****
04/19/2024 15:19:06 - INFO - __main__ -     Num examples = 172
04/19/2024 15:19:06 - INFO - __main__ -     Batch size = 64
04/19/2024 15:19:07 - INFO - __main__ -     eval_loss = 0.6893
04/19/2024 15:19:07 - INFO - __main__ -     eval_acc = 0.5407
04/19/2024 15:19:07 - INFO - __main__ -     ********************
04/19/2024 15:19:07 - INFO - __main__ -     Best acc:0.5407
04/19/2024 15:19:07 - INFO - __main__ -     ********************
04/19/2024 15:19:07 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/py_model.bin
epoch 0 step 26 loss 0.70064: 100%|██████████| 26/26 [00:17<00:00,  1.90s/it]epoch 0 step 26 loss 0.70064: 100%|██████████| 26/26 [00:17<00:00,  1.47it/s]
  0%|          | 0/26 [00:00<?, ?it/s]epoch 1 step 1 loss 0.71919:   0%|          | 0/26 [00:00<?, ?it/s]epoch 1 step 1 loss 0.71919:   4%|▍         | 1/26 [00:00<00:18,  1.35it/s]epoch 1 step 2 loss 0.72005:   4%|▍         | 1/26 [00:01<00:18,  1.35it/s]epoch 1 step 2 loss 0.72005:   8%|▊         | 2/26 [00:01<00:12,  1.97it/s]epoch 1 step 3 loss 0.71791:   8%|▊         | 2/26 [00:01<00:12,  1.97it/s]epoch 1 step 3 loss 0.71791:  12%|█▏        | 3/26 [00:01<00:09,  2.32it/s]epoch 1 step 4 loss 0.71449:  12%|█▏        | 3/26 [00:01<00:09,  2.32it/s]epoch 1 step 4 loss 0.71449:  15%|█▌        | 4/26 [00:01<00:08,  2.52it/s]epoch 1 step 5 loss 0.7064:  15%|█▌        | 4/26 [00:02<00:08,  2.52it/s] epoch 1 step 5 loss 0.7064:  19%|█▉        | 5/26 [00:02<00:07,  2.65it/s]epoch 1 step 6 loss 0.70383:  19%|█▉        | 5/26 [00:02<00:07,  2.65it/s]epoch 1 step 6 loss 0.70383:  23%|██▎       | 6/26 [00:02<00:07,  2.73it/s]epoch 1 step 7 loss 0.7038:  23%|██▎       | 6/26 [00:02<00:07,  2.73it/s] epoch 1 step 7 loss 0.7038:  27%|██▋       | 7/26 [00:02<00:06,  2.79it/s]epoch 1 step 8 loss 0.70049:  27%|██▋       | 7/26 [00:03<00:06,  2.79it/s]epoch 1 step 8 loss 0.70049:  31%|███       | 8/26 [00:03<00:06,  2.83it/s]epoch 1 step 9 loss 0.69897:  31%|███       | 8/26 [00:03<00:06,  2.83it/s]epoch 1 step 9 loss 0.69897:  35%|███▍      | 9/26 [00:03<00:05,  2.85it/s]epoch 1 step 10 loss 0.69891:  35%|███▍      | 9/26 [00:03<00:05,  2.85it/s]epoch 1 step 10 loss 0.69891:  38%|███▊      | 10/26 [00:03<00:05,  2.87it/s]epoch 1 step 11 loss 0.69624:  38%|███▊      | 10/26 [00:04<00:05,  2.87it/s]epoch 1 step 11 loss 0.69624:  42%|████▏     | 11/26 [00:04<00:05,  2.88it/s]epoch 1 step 12 loss 0.69506:  42%|████▏     | 11/26 [00:04<00:05,  2.88it/s]epoch 1 step 12 loss 0.69506:  46%|████▌     | 12/26 [00:04<00:04,  2.88it/s]epoch 1 step 13 loss 0.69297:  46%|████▌     | 12/26 [00:04<00:04,  2.88it/s]epoch 1 step 13 loss 0.69297:  50%|█████     | 13/26 [00:04<00:04,  2.89it/s]epoch 1 step 14 loss 0.69183:  50%|█████     | 13/26 [00:05<00:04,  2.89it/s]epoch 1 step 14 loss 0.69183:  54%|█████▍    | 14/26 [00:05<00:04,  2.90it/s]epoch 1 step 15 loss 0.69354:  54%|█████▍    | 14/26 [00:05<00:04,  2.90it/s]epoch 1 step 15 loss 0.69354:  58%|█████▊    | 15/26 [00:05<00:03,  2.88it/s]epoch 1 step 16 loss 0.69531:  58%|█████▊    | 15/26 [00:05<00:03,  2.88it/s]epoch 1 step 16 loss 0.69531:  62%|██████▏   | 16/26 [00:05<00:03,  2.87it/s]epoch 1 step 17 loss 0.69608:  62%|██████▏   | 16/26 [00:06<00:03,  2.87it/s]epoch 1 step 17 loss 0.69608:  65%|██████▌   | 17/26 [00:06<00:03,  2.88it/s]epoch 1 step 18 loss 0.6939:  65%|██████▌   | 17/26 [00:06<00:03,  2.88it/s] epoch 1 step 18 loss 0.6939:  69%|██████▉   | 18/26 [00:06<00:02,  2.90it/s]epoch 1 step 19 loss 0.69242:  69%|██████▉   | 18/26 [00:06<00:02,  2.90it/s]epoch 1 step 19 loss 0.69242:  73%|███████▎  | 19/26 [00:06<00:02,  2.90it/s]epoch 1 step 20 loss 0.69302:  73%|███████▎  | 19/26 [00:07<00:02,  2.90it/s]epoch 1 step 20 loss 0.69302:  77%|███████▋  | 20/26 [00:07<00:02,  2.90it/s]epoch 1 step 21 loss 0.69308:  77%|███████▋  | 20/26 [00:07<00:02,  2.90it/s]epoch 1 step 21 loss 0.69308:  81%|████████  | 21/26 [00:07<00:01,  2.90it/s]epoch 1 step 22 loss 0.69226:  81%|████████  | 21/26 [00:07<00:01,  2.90it/s]epoch 1 step 22 loss 0.69226:  85%|████████▍ | 22/26 [00:07<00:01,  2.91it/s]epoch 1 step 23 loss 0.69271:  85%|████████▍ | 22/26 [00:08<00:01,  2.91it/s]epoch 1 step 23 loss 0.69271:  88%|████████▊ | 23/26 [00:08<00:01,  2.90it/s]epoch 1 step 24 loss 0.69296:  88%|████████▊ | 23/26 [00:08<00:01,  2.90it/s]epoch 1 step 24 loss 0.69296:  92%|█████████▏| 24/26 [00:08<00:00,  2.90it/s]epoch 1 step 25 loss 0.69351:  92%|█████████▏| 24/26 [00:08<00:00,  2.90it/s]epoch 1 step 25 loss 0.69351:  96%|█████████▌| 25/26 [00:09<00:00,  2.89it/s]epoch 1 step 26 loss 0.69372:  96%|█████████▌| 25/26 [00:09<00:00,  2.89it/s]04/19/2024 15:19:20 - INFO - __main__ -   ***** Running evaluation *****
04/19/2024 15:19:20 - INFO - __main__ -     Num examples = 172
04/19/2024 15:19:20 - INFO - __main__ -     Batch size = 64
04/19/2024 15:19:21 - INFO - __main__ -     eval_loss = 0.6769
04/19/2024 15:19:21 - INFO - __main__ -     eval_acc = 0.5407
epoch 1 step 26 loss 0.69372: 100%|██████████| 26/26 [00:13<00:00,  1.64s/it]epoch 1 step 26 loss 0.69372: 100%|██████████| 26/26 [00:13<00:00,  1.89it/s]
  0%|          | 0/26 [00:00<?, ?it/s]epoch 2 step 1 loss 0.66516:   0%|          | 0/26 [00:00<?, ?it/s]epoch 2 step 1 loss 0.66516:   4%|▍         | 1/26 [00:00<00:20,  1.23it/s]epoch 2 step 2 loss 0.66392:   4%|▍         | 1/26 [00:01<00:20,  1.23it/s]epoch 2 step 2 loss 0.66392:   8%|▊         | 2/26 [00:01<00:12,  1.86it/s]epoch 2 step 3 loss 0.67596:   8%|▊         | 2/26 [00:01<00:12,  1.86it/s]epoch 2 step 3 loss 0.67596:  12%|█▏        | 3/26 [00:01<00:10,  2.23it/s]epoch 2 step 4 loss 0.67827:  12%|█▏        | 3/26 [00:01<00:10,  2.23it/s]epoch 2 step 4 loss 0.67827:  15%|█▌        | 4/26 [00:01<00:08,  2.45it/s]epoch 2 step 5 loss 0.67967:  15%|█▌        | 4/26 [00:02<00:08,  2.45it/s]epoch 2 step 5 loss 0.67967:  19%|█▉        | 5/26 [00:02<00:08,  2.60it/s]epoch 2 step 6 loss 0.6773:  19%|█▉        | 5/26 [00:02<00:08,  2.60it/s] epoch 2 step 6 loss 0.6773:  23%|██▎       | 6/26 [00:02<00:07,  2.69it/s]epoch 2 step 7 loss 0.6788:  23%|██▎       | 6/26 [00:02<00:07,  2.69it/s]epoch 2 step 7 loss 0.6788:  27%|██▋       | 7/26 [00:02<00:06,  2.76it/s]epoch 2 step 8 loss 0.67642:  27%|██▋       | 7/26 [00:03<00:06,  2.76it/s]epoch 2 step 8 loss 0.67642:  31%|███       | 8/26 [00:03<00:06,  2.80it/s]epoch 2 step 9 loss 0.67718:  31%|███       | 8/26 [00:03<00:06,  2.80it/s]epoch 2 step 9 loss 0.67718:  35%|███▍      | 9/26 [00:03<00:05,  2.84it/s]epoch 2 step 10 loss 0.67826:  35%|███▍      | 9/26 [00:03<00:05,  2.84it/s]epoch 2 step 10 loss 0.67826:  38%|███▊      | 10/26 [00:03<00:05,  2.86it/s]epoch 2 step 11 loss 0.68216:  38%|███▊      | 10/26 [00:04<00:05,  2.86it/s]epoch 2 step 11 loss 0.68216:  42%|████▏     | 11/26 [00:04<00:05,  2.88it/s]epoch 2 step 12 loss 0.68205:  42%|████▏     | 11/26 [00:04<00:05,  2.88it/s]epoch 2 step 12 loss 0.68205:  46%|████▌     | 12/26 [00:04<00:04,  2.88it/s]epoch 2 step 13 loss 0.67971:  46%|████▌     | 12/26 [00:04<00:04,  2.88it/s]epoch 2 step 13 loss 0.67971:  50%|█████     | 13/26 [00:04<00:04,  2.88it/s]epoch 2 step 14 loss 0.67962:  50%|█████     | 13/26 [00:05<00:04,  2.88it/s]epoch 2 step 14 loss 0.67962:  54%|█████▍    | 14/26 [00:05<00:04,  2.87it/s]epoch 2 step 15 loss 0.67931:  54%|█████▍    | 14/26 [00:05<00:04,  2.87it/s]epoch 2 step 15 loss 0.67931:  58%|█████▊    | 15/26 [00:05<00:03,  2.88it/s]epoch 2 step 16 loss 0.67879:  58%|█████▊    | 15/26 [00:05<00:03,  2.88it/s]epoch 2 step 16 loss 0.67879:  62%|██████▏   | 16/26 [00:05<00:03,  2.88it/s]epoch 2 step 17 loss 0.67607:  62%|██████▏   | 16/26 [00:06<00:03,  2.88it/s]epoch 2 step 17 loss 0.67607:  65%|██████▌   | 17/26 [00:06<00:03,  2.89it/s]epoch 2 step 18 loss 0.67433:  65%|██████▌   | 17/26 [00:06<00:03,  2.89it/s]epoch 2 step 18 loss 0.67433:  69%|██████▉   | 18/26 [00:06<00:02,  2.89it/s]epoch 2 step 19 loss 0.67471:  69%|██████▉   | 18/26 [00:07<00:02,  2.89it/s]epoch 2 step 19 loss 0.67471:  73%|███████▎  | 19/26 [00:07<00:02,  2.90it/s]epoch 2 step 20 loss 0.67637:  73%|███████▎  | 19/26 [00:07<00:02,  2.90it/s]epoch 2 step 20 loss 0.67637:  77%|███████▋  | 20/26 [00:07<00:02,  2.90it/s]epoch 2 step 21 loss 0.67603:  77%|███████▋  | 20/26 [00:07<00:02,  2.90it/s]epoch 2 step 21 loss 0.67603:  81%|████████  | 21/26 [00:07<00:01,  2.90it/s]epoch 2 step 22 loss 0.67697:  81%|████████  | 21/26 [00:08<00:01,  2.90it/s]epoch 2 step 22 loss 0.67697:  85%|████████▍ | 22/26 [00:08<00:01,  2.90it/s]epoch 2 step 23 loss 0.67595:  85%|████████▍ | 22/26 [00:08<00:01,  2.90it/s]epoch 2 step 23 loss 0.67595:  88%|████████▊ | 23/26 [00:08<00:01,  2.88it/s]epoch 2 step 24 loss 0.67601:  88%|████████▊ | 23/26 [00:08<00:01,  2.88it/s]epoch 2 step 24 loss 0.67601:  92%|█████████▏| 24/26 [00:08<00:00,  2.88it/s]epoch 2 step 25 loss 0.67795:  92%|█████████▏| 24/26 [00:09<00:00,  2.88it/s]epoch 2 step 25 loss 0.67795:  96%|█████████▌| 25/26 [00:09<00:00,  2.88it/s]epoch 2 step 26 loss 0.67803:  96%|█████████▌| 25/26 [00:09<00:00,  2.88it/s]04/19/2024 15:19:34 - INFO - __main__ -   ***** Running evaluation *****
04/19/2024 15:19:34 - INFO - __main__ -     Num examples = 172
04/19/2024 15:19:34 - INFO - __main__ -     Batch size = 64
04/19/2024 15:19:35 - INFO - __main__ -     eval_loss = 0.6718
04/19/2024 15:19:35 - INFO - __main__ -     eval_acc = 0.5407
epoch 2 step 26 loss 0.67803: 100%|██████████| 26/26 [00:13<00:00,  1.66s/it]epoch 2 step 26 loss 0.67803: 100%|██████████| 26/26 [00:13<00:00,  1.87it/s]
  0%|          | 0/26 [00:00<?, ?it/s]epoch 3 step 1 loss 0.69155:   0%|          | 0/26 [00:00<?, ?it/s]epoch 3 step 1 loss 0.69155:   4%|▍         | 1/26 [00:00<00:18,  1.36it/s]epoch 3 step 2 loss 0.69023:   4%|▍         | 1/26 [00:01<00:18,  1.36it/s]epoch 3 step 2 loss 0.69023:   8%|▊         | 2/26 [00:01<00:12,  1.98it/s]epoch 3 step 3 loss 0.68612:   8%|▊         | 2/26 [00:01<00:12,  1.98it/s]epoch 3 step 3 loss 0.68612:  12%|█▏        | 3/26 [00:01<00:09,  2.32it/s]epoch 3 step 4 loss 0.69756:  12%|█▏        | 3/26 [00:01<00:09,  2.32it/s]epoch 3 step 4 loss 0.69756:  15%|█▌        | 4/26 [00:01<00:08,  2.52it/s]epoch 3 step 5 loss 0.68723:  15%|█▌        | 4/26 [00:02<00:08,  2.52it/s]epoch 3 step 5 loss 0.68723:  19%|█▉        | 5/26 [00:02<00:07,  2.64it/s]epoch 3 step 6 loss 0.6823:  19%|█▉        | 5/26 [00:02<00:07,  2.64it/s] epoch 3 step 6 loss 0.6823:  23%|██▎       | 6/26 [00:02<00:07,  2.72it/s]epoch 3 step 7 loss 0.67703:  23%|██▎       | 6/26 [00:02<00:07,  2.72it/s]epoch 3 step 7 loss 0.67703:  27%|██▋       | 7/26 [00:02<00:06,  2.77it/s]epoch 3 step 8 loss 0.67786:  27%|██▋       | 7/26 [00:03<00:06,  2.77it/s]epoch 3 step 8 loss 0.67786:  31%|███       | 8/26 [00:03<00:06,  2.80it/s]epoch 3 step 9 loss 0.67958:  31%|███       | 8/26 [00:03<00:06,  2.80it/s]epoch 3 step 9 loss 0.67958:  35%|███▍      | 9/26 [00:03<00:05,  2.84it/s]epoch 3 step 10 loss 0.67957:  35%|███▍      | 9/26 [00:03<00:05,  2.84it/s]epoch 3 step 10 loss 0.67957:  38%|███▊      | 10/26 [00:03<00:05,  2.86it/s]epoch 3 step 11 loss 0.67707:  38%|███▊      | 10/26 [00:04<00:05,  2.86it/s]epoch 3 step 11 loss 0.67707:  42%|████▏     | 11/26 [00:04<00:05,  2.87it/s]epoch 3 step 12 loss 0.6767:  42%|████▏     | 11/26 [00:04<00:05,  2.87it/s] epoch 3 step 12 loss 0.6767:  46%|████▌     | 12/26 [00:04<00:04,  2.88it/s]epoch 3 step 13 loss 0.67575:  46%|████▌     | 12/26 [00:04<00:04,  2.88it/s]epoch 3 step 13 loss 0.67575:  50%|█████     | 13/26 [00:04<00:04,  2.89it/s]epoch 3 step 14 loss 0.67291:  50%|█████     | 13/26 [00:05<00:04,  2.89it/s]epoch 3 step 14 loss 0.67291:  54%|█████▍    | 14/26 [00:05<00:04,  2.89it/s]epoch 3 step 15 loss 0.67386:  54%|█████▍    | 14/26 [00:05<00:04,  2.89it/s]epoch 3 step 15 loss 0.67386:  58%|█████▊    | 15/26 [00:05<00:03,  2.90it/s]epoch 3 step 16 loss 0.6754:  58%|█████▊    | 15/26 [00:05<00:03,  2.90it/s] epoch 3 step 16 loss 0.6754:  62%|██████▏   | 16/26 [00:05<00:03,  2.89it/s]epoch 3 step 17 loss 0.67494:  62%|██████▏   | 16/26 [00:06<00:03,  2.89it/s]epoch 3 step 17 loss 0.67494:  65%|██████▌   | 17/26 [00:06<00:03,  2.88it/s]epoch 3 step 18 loss 0.67389:  65%|██████▌   | 17/26 [00:06<00:03,  2.88it/s]epoch 3 step 18 loss 0.67389:  69%|██████▉   | 18/26 [00:06<00:02,  2.88it/s]epoch 3 step 19 loss 0.67241:  69%|██████▉   | 18/26 [00:06<00:02,  2.88it/s]epoch 3 step 19 loss 0.67241:  73%|███████▎  | 19/26 [00:06<00:02,  2.89it/s]epoch 3 step 20 loss 0.67506:  73%|███████▎  | 19/26 [00:07<00:02,  2.89it/s]epoch 3 step 20 loss 0.67506:  77%|███████▋  | 20/26 [00:07<00:02,  2.89it/s]epoch 3 step 21 loss 0.67516:  77%|███████▋  | 20/26 [00:07<00:02,  2.89it/s]epoch 3 step 21 loss 0.67516:  81%|████████  | 21/26 [00:07<00:01,  2.88it/s]epoch 3 step 22 loss 0.67332:  81%|████████  | 21/26 [00:07<00:01,  2.88it/s]epoch 3 step 22 loss 0.67332:  85%|████████▍ | 22/26 [00:07<00:01,  2.89it/s]epoch 3 step 23 loss 0.67276:  85%|████████▍ | 22/26 [00:08<00:01,  2.89it/s]epoch 3 step 23 loss 0.67276:  88%|████████▊ | 23/26 [00:08<00:01,  2.90it/s]epoch 3 step 24 loss 0.67344:  88%|████████▊ | 23/26 [00:08<00:01,  2.90it/s]epoch 3 step 24 loss 0.67344:  92%|█████████▏| 24/26 [00:08<00:00,  2.90it/s]epoch 3 step 25 loss 0.67298:  92%|█████████▏| 24/26 [00:09<00:00,  2.90it/s]epoch 3 step 25 loss 0.67298:  96%|█████████▌| 25/26 [00:09<00:00,  2.90it/s]epoch 3 step 26 loss 0.67421:  96%|█████████▌| 25/26 [00:09<00:00,  2.90it/s]04/19/2024 15:19:47 - INFO - __main__ -   ***** Running evaluation *****
04/19/2024 15:19:47 - INFO - __main__ -     Num examples = 172
04/19/2024 15:19:47 - INFO - __main__ -     Batch size = 64
04/19/2024 15:19:49 - INFO - __main__ -     eval_loss = 0.6665
04/19/2024 15:19:49 - INFO - __main__ -     eval_acc = 0.5233
epoch 3 step 26 loss 0.67421: 100%|██████████| 26/26 [00:13<00:00,  1.63s/it]epoch 3 step 26 loss 0.67421: 100%|██████████| 26/26 [00:13<00:00,  1.89it/s]
  0%|          | 0/26 [00:00<?, ?it/s]epoch 4 step 1 loss 0.654:   0%|          | 0/26 [00:00<?, ?it/s]epoch 4 step 1 loss 0.654:   4%|▍         | 1/26 [00:00<00:18,  1.37it/s]epoch 4 step 2 loss 0.64182:   4%|▍         | 1/26 [00:01<00:18,  1.37it/s]epoch 4 step 2 loss 0.64182:   8%|▊         | 2/26 [00:01<00:13,  1.74it/s]epoch 4 step 3 loss 0.66027:   8%|▊         | 2/26 [00:01<00:13,  1.74it/s]epoch 4 step 3 loss 0.66027:  12%|█▏        | 3/26 [00:01<00:10,  2.13it/s]epoch 4 step 4 loss 0.65614:  12%|█▏        | 3/26 [00:01<00:10,  2.13it/s]epoch 4 step 4 loss 0.65614:  15%|█▌        | 4/26 [00:01<00:09,  2.38it/s]epoch 4 step 5 loss 0.64882:  15%|█▌        | 4/26 [00:02<00:09,  2.38it/s]epoch 4 step 5 loss 0.64882:  19%|█▉        | 5/26 [00:02<00:08,  2.55it/s]epoch 4 step 6 loss 0.65622:  19%|█▉        | 5/26 [00:02<00:08,  2.55it/s]epoch 4 step 6 loss 0.65622:  23%|██▎       | 6/26 [00:02<00:07,  2.66it/s]epoch 4 step 7 loss 0.65422:  23%|██▎       | 6/26 [00:02<00:07,  2.66it/s]epoch 4 step 7 loss 0.65422:  27%|██▋       | 7/26 [00:02<00:06,  2.74it/s]epoch 4 step 8 loss 0.65877:  27%|██▋       | 7/26 [00:03<00:06,  2.74it/s]epoch 4 step 8 loss 0.65877:  31%|███       | 8/26 [00:03<00:06,  2.79it/s]epoch 4 step 9 loss 0.66152:  31%|███       | 8/26 [00:03<00:06,  2.79it/s]epoch 4 step 9 loss 0.66152:  35%|███▍      | 9/26 [00:03<00:06,  2.82it/s]epoch 4 step 10 loss 0.66165:  35%|███▍      | 9/26 [00:03<00:06,  2.82it/s]epoch 4 step 10 loss 0.66165:  38%|███▊      | 10/26 [00:03<00:05,  2.83it/s]epoch 4 step 11 loss 0.66142:  38%|███▊      | 10/26 [00:04<00:05,  2.83it/s]epoch 4 step 11 loss 0.66142:  42%|████▏     | 11/26 [00:04<00:05,  2.84it/s]epoch 4 step 12 loss 0.66155:  42%|████▏     | 11/26 [00:04<00:05,  2.84it/s]epoch 4 step 12 loss 0.66155:  46%|████▌     | 12/26 [00:04<00:04,  2.85it/s]epoch 4 step 13 loss 0.66103:  46%|████▌     | 12/26 [00:04<00:04,  2.85it/s]epoch 4 step 13 loss 0.66103:  50%|█████     | 13/26 [00:04<00:04,  2.86it/s]epoch 4 step 14 loss 0.66045:  50%|█████     | 13/26 [00:05<00:04,  2.86it/s]epoch 4 step 14 loss 0.66045:  54%|█████▍    | 14/26 [00:05<00:04,  2.87it/s]epoch 4 step 15 loss 0.66169:  54%|█████▍    | 14/26 [00:05<00:04,  2.87it/s]epoch 4 step 15 loss 0.66169:  58%|█████▊    | 15/26 [00:05<00:03,  2.88it/s]epoch 4 step 16 loss 0.66176:  58%|█████▊    | 15/26 [00:06<00:03,  2.88it/s]epoch 4 step 16 loss 0.66176:  62%|██████▏   | 16/26 [00:06<00:03,  2.88it/s]epoch 4 step 17 loss 0.6626:  62%|██████▏   | 16/26 [00:06<00:03,  2.88it/s] epoch 4 step 17 loss 0.6626:  65%|██████▌   | 17/26 [00:06<00:03,  2.88it/s]epoch 4 step 18 loss 0.66562:  65%|██████▌   | 17/26 [00:06<00:03,  2.88it/s]epoch 4 step 18 loss 0.66562:  69%|██████▉   | 18/26 [00:06<00:02,  2.88it/s]epoch 4 step 19 loss 0.66621:  69%|██████▉   | 18/26 [00:07<00:02,  2.88it/s]epoch 4 step 19 loss 0.66621:  73%|███████▎  | 19/26 [00:07<00:02,  2.89it/s]epoch 4 step 20 loss 0.66813:  73%|███████▎  | 19/26 [00:07<00:02,  2.89it/s]epoch 4 step 20 loss 0.66813:  77%|███████▋  | 20/26 [00:07<00:02,  2.88it/s]epoch 4 step 21 loss 0.66851:  77%|███████▋  | 20/26 [00:07<00:02,  2.88it/s]epoch 4 step 21 loss 0.66851:  81%|████████  | 21/26 [00:07<00:01,  2.88it/s]epoch 4 step 22 loss 0.66944:  81%|████████  | 21/26 [00:08<00:01,  2.88it/s]epoch 4 step 22 loss 0.66944:  85%|████████▍ | 22/26 [00:08<00:01,  2.88it/s]epoch 4 step 23 loss 0.66956:  85%|████████▍ | 22/26 [00:08<00:01,  2.88it/s]epoch 4 step 23 loss 0.66956:  88%|████████▊ | 23/26 [00:08<00:01,  2.87it/s]epoch 4 step 24 loss 0.67034:  88%|████████▊ | 23/26 [00:08<00:01,  2.87it/s]epoch 4 step 24 loss 0.67034:  92%|█████████▏| 24/26 [00:08<00:00,  2.88it/s]epoch 4 step 25 loss 0.67078:  92%|█████████▏| 24/26 [00:09<00:00,  2.88it/s]epoch 4 step 25 loss 0.67078:  96%|█████████▌| 25/26 [00:09<00:00,  2.87it/s]epoch 4 step 26 loss 0.6719:  96%|█████████▌| 25/26 [00:09<00:00,  2.87it/s] 04/19/2024 15:20:01 - INFO - __main__ -   ***** Running evaluation *****
04/19/2024 15:20:01 - INFO - __main__ -     Num examples = 172
04/19/2024 15:20:01 - INFO - __main__ -     Batch size = 64
04/19/2024 15:20:02 - INFO - __main__ -     eval_loss = 0.6655
04/19/2024 15:20:02 - INFO - __main__ -     eval_acc = 0.5349
epoch 4 step 26 loss 0.6719: 100%|██████████| 26/26 [00:13<00:00,  1.63s/it]epoch 4 step 26 loss 0.6719: 100%|██████████| 26/26 [00:13<00:00,  1.88it/s]
