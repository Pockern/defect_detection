04/21/2024 00:05:30 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../data/temp.jsonl', output_dir='./saved_models', language_type='cpp', eval_data_file='../data/temp.jsonl', test_data_file='../data/temp.jsonl', model_name_or_path='../../huggingface_models/microsoft/codebert-base/', block_size=400, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, train_batch_size=1, eval_batch_size=1, gradient_accumulation_steps=4, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, epoch=3, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, seed=123456, min_loss_delta=0.001, dropout_probability=0, device=device(type='cuda'), n_gpu=2, start_epoch=0, start_step=0)
04/21/2024 00:05:34 - INFO - __main__ -   ***** Running training *****
04/21/2024 00:05:34 - INFO - __main__ -     Num examples = 62
04/21/2024 00:05:34 - INFO - __main__ -     Num Epochs = 3
04/21/2024 00:05:34 - INFO - __main__ -     Batch size = 1
04/21/2024 00:05:34 - INFO - __main__ -     Total train batch size = 4
04/21/2024 00:05:34 - INFO - __main__ -     Gradient Accumulation steps = 4
04/21/2024 00:05:34 - INFO - __main__ -     Total optimization steps = 186
  0%|          | 0/62 [00:00<?, ?it/s]epoch 0 step 1 loss 0.62239:   0%|          | 0/62 [00:02<?, ?it/s]epoch 0 step 1 loss 0.62239:   2%|▏         | 1/62 [00:02<02:05,  2.07s/it]epoch 0 step 2 loss 0.69332:   2%|▏         | 1/62 [00:02<02:05,  2.07s/it]epoch 0 step 2 loss 0.69332:   3%|▎         | 2/62 [00:02<01:11,  1.19s/it]epoch 0 step 3 loss 0.67388:   3%|▎         | 2/62 [00:02<01:11,  1.19s/it]epoch 0 step 3 loss 0.67388:   5%|▍         | 3/62 [00:02<00:44,  1.32it/s]epoch 0 step 4 loss 0.69213:   5%|▍         | 3/62 [00:03<00:44,  1.32it/s]epoch 0 step 4 loss 0.69213:   6%|▋         | 4/62 [00:03<00:31,  1.85it/s]epoch 0 step 5 loss 0.70306:   6%|▋         | 4/62 [00:03<00:31,  1.85it/s]epoch 0 step 5 loss 0.70306:   8%|▊         | 5/62 [00:03<00:26,  2.13it/s]epoch 0 step 6 loss 0.71903:   8%|▊         | 5/62 [00:04<00:26,  2.13it/s]epoch 0 step 6 loss 0.71903:  10%|▉         | 6/62 [00:04<00:35,  1.58it/s]epoch 0 step 7 loss 0.72649:  10%|▉         | 6/62 [00:05<00:35,  1.58it/s]epoch 0 step 7 loss 0.72649:  11%|█▏        | 7/62 [00:05<00:36,  1.49it/s]epoch 0 step 8 loss 0.73305:  11%|█▏        | 7/62 [00:05<00:36,  1.49it/s]epoch 0 step 8 loss 0.73305:  13%|█▎        | 8/62 [00:06<00:39,  1.36it/s]epoch 0 step 9 loss 0.73755:  13%|█▎        | 8/62 [00:06<00:39,  1.36it/s]epoch 0 step 9 loss 0.73755:  15%|█▍        | 9/62 [00:06<00:37,  1.41it/s]epoch 0 step 10 loss 0.72595:  15%|█▍        | 9/62 [00:06<00:37,  1.41it/s]epoch 0 step 10 loss 0.72595:  16%|█▌        | 10/62 [00:06<00:31,  1.68it/s]epoch 0 step 11 loss 0.72928:  16%|█▌        | 10/62 [00:07<00:31,  1.68it/s]epoch 0 step 12 loss 0.73366:  16%|█▌        | 10/62 [00:07<00:31,  1.68it/s]epoch 0 step 12 loss 0.73366:  19%|█▉        | 12/62 [00:07<00:20,  2.47it/s]epoch 0 step 13 loss 0.73512:  19%|█▉        | 12/62 [00:07<00:20,  2.47it/s]epoch 0 step 13 loss 0.73512:  21%|██        | 13/62 [00:07<00:20,  2.34it/s]epoch 0 step 14 loss 0.73621:  21%|██        | 13/62 [00:08<00:20,  2.34it/s]epoch 0 step 14 loss 0.73621:  23%|██▎       | 14/62 [00:08<00:21,  2.21it/s]epoch 0 step 15 loss 0.73011:  23%|██▎       | 14/62 [00:08<00:21,  2.21it/s]epoch 0 step 16 loss 0.73069:  23%|██▎       | 14/62 [00:09<00:21,  2.21it/s]epoch 0 step 16 loss 0.73069:  26%|██▌       | 16/62 [00:09<00:18,  2.44it/s]epoch 0 step 17 loss 0.72461:  26%|██▌       | 16/62 [00:09<00:18,  2.44it/s]epoch 0 step 18 loss 0.72425:  26%|██▌       | 16/62 [00:09<00:18,  2.44it/s]epoch 0 step 18 loss 0.72425:  29%|██▉       | 18/62 [00:09<00:13,  3.32it/s]epoch 0 step 19 loss 0.71976:  29%|██▉       | 18/62 [00:10<00:13,  3.32it/s]epoch 0 step 19 loss 0.71976:  31%|███       | 19/62 [00:10<00:18,  2.37it/s]epoch 0 step 20 loss 0.71977:  31%|███       | 19/62 [00:10<00:18,  2.37it/s]epoch 0 step 20 loss 0.71977:  32%|███▏      | 20/62 [00:10<00:16,  2.56it/s]epoch 0 step 21 loss 0.72002:  32%|███▏      | 20/62 [00:11<00:16,  2.56it/s]epoch 0 step 21 loss 0.72002:  34%|███▍      | 21/62 [00:11<00:20,  1.99it/s]epoch 0 step 22 loss 0.71857:  34%|███▍      | 21/62 [00:11<00:20,  1.99it/s]epoch 0 step 22 loss 0.71857:  35%|███▌      | 22/62 [00:11<00:21,  1.89it/s]epoch 0 step 23 loss 0.71703:  35%|███▌      | 22/62 [00:12<00:21,  1.89it/s]epoch 0 step 23 loss 0.71703:  37%|███▋      | 23/62 [00:12<00:22,  1.70it/s]epoch 0 step 24 loss 0.71611:  37%|███▋      | 23/62 [00:13<00:22,  1.70it/s]epoch 0 step 24 loss 0.71611:  39%|███▊      | 24/62 [00:13<00:20,  1.82it/s]epoch 0 step 25 loss 0.71511:  39%|███▊      | 24/62 [00:13<00:20,  1.82it/s]epoch 0 step 25 loss 0.71511:  40%|████      | 25/62 [00:13<00:18,  2.01it/s]epoch 0 step 26 loss 0.71371:  40%|████      | 25/62 [00:14<00:18,  2.01it/s]epoch 0 step 26 loss 0.71371:  42%|████▏     | 26/62 [00:14<00:19,  1.88it/s]epoch 0 step 27 loss 0.71354:  42%|████▏     | 26/62 [00:14<00:19,  1.88it/s]epoch 0 step 28 loss 0.71209:  42%|████▏     | 26/62 [00:15<00:19,  1.88it/s]epoch 0 step 28 loss 0.71209:  45%|████▌     | 28/62 [00:15<00:17,  1.96it/s]epoch 0 step 29 loss 0.71182:  45%|████▌     | 28/62 [00:15<00:17,  1.96it/s]epoch 0 step 30 loss 0.71066:  45%|████▌     | 28/62 [00:15<00:17,  1.96it/s]epoch 0 step 30 loss 0.71066:  48%|████▊     | 30/62 [00:15<00:13,  2.46it/s]epoch 0 step 31 loss 0.70836:  48%|████▊     | 30/62 [00:15<00:13,  2.46it/s]epoch 0 step 32 loss 0.70723:  48%|████▊     | 30/62 [00:15<00:13,  2.46it/s]epoch 0 step 32 loss 0.70723:  52%|█████▏    | 32/62 [00:15<00:09,  3.10it/s]epoch 0 step 33 loss 0.70716:  52%|█████▏    | 32/62 [00:16<00:09,  3.10it/s]epoch 0 step 33 loss 0.70716:  53%|█████▎    | 33/62 [00:16<00:12,  2.25it/s]epoch 0 step 34 loss 0.70753:  53%|█████▎    | 33/62 [00:17<00:12,  2.25it/s]epoch 0 step 34 loss 0.70753:  55%|█████▍    | 34/62 [00:17<00:10,  2.57it/s]epoch 0 step 35 loss 0.70601:  55%|█████▍    | 34/62 [00:17<00:10,  2.57it/s]epoch 0 step 36 loss 0.7067:  55%|█████▍    | 34/62 [00:17<00:10,  2.57it/s] epoch 0 step 36 loss 0.7067:  58%|█████▊    | 36/62 [00:17<00:07,  3.69it/s]epoch 0 step 37 loss 0.70695:  58%|█████▊    | 36/62 [00:18<00:07,  3.69it/s]epoch 0 step 37 loss 0.70695:  60%|█████▉    | 37/62 [00:18<00:10,  2.48it/s]epoch 0 step 38 loss 0.70592:  60%|█████▉    | 37/62 [00:18<00:10,  2.48it/s]epoch 0 step 38 loss 0.70592:  61%|██████▏   | 38/62 [00:18<00:08,  2.74it/s]epoch 0 step 39 loss 0.70504:  61%|██████▏   | 38/62 [00:18<00:08,  2.74it/s]epoch 0 step 39 loss 0.70504:  63%|██████▎   | 39/62 [00:18<00:09,  2.42it/s]epoch 0 step 40 loss 0.70681:  63%|██████▎   | 39/62 [00:18<00:09,  2.42it/s]epoch 0 step 41 loss 0.70622:  63%|██████▎   | 39/62 [00:19<00:09,  2.42it/s]epoch 0 step 41 loss 0.70622:  66%|██████▌   | 41/62 [00:19<00:09,  2.28it/s]epoch 0 step 42 loss 0.70666:  66%|██████▌   | 41/62 [00:20<00:09,  2.28it/s]epoch 0 step 42 loss 0.70666:  68%|██████▊   | 42/62 [00:20<00:08,  2.41it/s]epoch 0 step 43 loss 0.70636:  68%|██████▊   | 42/62 [00:20<00:08,  2.41it/s]epoch 0 step 43 loss 0.70636:  69%|██████▉   | 43/62 [00:20<00:08,  2.30it/s]epoch 0 step 44 loss 0.70679:  69%|██████▉   | 43/62 [00:21<00:08,  2.30it/s]epoch 0 step 44 loss 0.70679:  71%|███████   | 44/62 [00:21<00:08,  2.19it/s]epoch 0 step 45 loss 0.70725:  71%|███████   | 44/62 [00:21<00:08,  2.19it/s]epoch 0 step 45 loss 0.70725:  73%|███████▎  | 45/62 [00:21<00:06,  2.64it/s]epoch 0 step 46 loss 0.70662:  73%|███████▎  | 45/62 [00:22<00:06,  2.64it/s]epoch 0 step 46 loss 0.70662:  74%|███████▍  | 46/62 [00:22<00:08,  1.88it/s]epoch 0 step 47 loss 0.70674:  74%|███████▍  | 46/62 [00:22<00:08,  1.88it/s]epoch 0 step 47 loss 0.70674:  76%|███████▌  | 47/62 [00:22<00:06,  2.17it/s]epoch 0 step 48 loss 0.70642:  76%|███████▌  | 47/62 [00:23<00:06,  2.17it/s]epoch 0 step 48 loss 0.70642:  77%|███████▋  | 48/62 [00:23<00:07,  1.97it/s]epoch 0 step 49 loss 0.70588:  77%|███████▋  | 48/62 [00:23<00:07,  1.97it/s]epoch 0 step 49 loss 0.70588:  79%|███████▉  | 49/62 [00:23<00:05,  2.39it/s]epoch 0 step 50 loss 0.70589:  79%|███████▉  | 49/62 [00:24<00:05,  2.39it/s]epoch 0 step 50 loss 0.70589:  81%|████████  | 50/62 [00:24<00:05,  2.05it/s]epoch 0 step 51 loss 0.70689:  81%|████████  | 50/62 [00:24<00:05,  2.05it/s]epoch 0 step 52 loss 0.70703:  81%|████████  | 50/62 [00:24<00:05,  2.05it/s]epoch 0 step 52 loss 0.70703:  84%|████████▍ | 52/62 [00:24<00:04,  2.45it/s]epoch 0 step 53 loss 0.70667:  84%|████████▍ | 52/62 [00:25<00:04,  2.45it/s]epoch 0 step 53 loss 0.70667:  85%|████████▌ | 53/62 [00:25<00:03,  2.56it/s]epoch 0 step 54 loss 0.707:  85%|████████▌ | 53/62 [00:25<00:03,  2.56it/s]  epoch 0 step 54 loss 0.707:  87%|████████▋ | 54/62 [00:25<00:03,  2.28it/s]epoch 0 step 55 loss 0.70639:  87%|████████▋ | 54/62 [00:25<00:03,  2.28it/s]epoch 0 step 55 loss 0.70639:  89%|████████▊ | 55/62 [00:25<00:02,  2.76it/s]epoch 0 step 56 loss 0.70692:  89%|████████▊ | 55/62 [00:26<00:02,  2.76it/s]epoch 0 step 56 loss 0.70692:  90%|█████████ | 56/62 [00:26<00:02,  2.76it/s]epoch 0 step 57 loss 0.70608:  90%|█████████ | 56/62 [00:26<00:02,  2.76it/s]epoch 0 step 58 loss 0.70635:  90%|█████████ | 56/62 [00:26<00:02,  2.76it/s]epoch 0 step 58 loss 0.70635:  94%|█████████▎| 58/62 [00:26<00:01,  3.55it/s]epoch 0 step 59 loss 0.70615:  94%|█████████▎| 58/62 [00:27<00:01,  3.55it/s]epoch 0 step 59 loss 0.70615:  95%|█████████▌| 59/62 [00:27<00:01,  2.40it/s]epoch 0 step 60 loss 0.70569:  95%|█████████▌| 59/62 [00:27<00:01,  2.40it/s]04/21/2024 00:06:04 - INFO - __main__ -   ***** Running evaluation *****
04/21/2024 00:06:04 - INFO - __main__ -     Num examples = 62
04/21/2024 00:06:04 - INFO - __main__ -     Batch size = 1
04/21/2024 00:06:13 - INFO - __main__ -     eval_loss = 0.6916
04/21/2024 00:06:13 - INFO - __main__ -     eval_acc = 0.5161
epoch 0 step 60 loss 0.70569:  95%|█████████▌| 59/62 [00:38<00:01,  1.53it/s]
