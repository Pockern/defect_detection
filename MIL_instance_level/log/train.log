04/23/2024 00:52:03 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../data/cpp/train.jsonl', output_dir='./saved_models', language_type='cpp', eval_data_file='../data/cpp/valid.jsonl', test_data_file='../data/cpp/test.jsonl', model_name_or_path='../../huggingface_models/microsoft/codebert-base/', block_size=400, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, train_batch_size=1, eval_batch_size=1, gradient_accumulation_steps=8, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, epoch=20, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, seed=123456, min_loss_delta=0.001, dropout_probability=0, device=device(type='cuda'), n_gpu=1, start_epoch=0, start_step=0)
04/23/2024 00:52:06 - INFO - __main__ -   *** Example ***
04/23/2024 00:52:06 - INFO - __main__ -   file_id: CWE-665/bad_4751_0
04/23/2024 00:52:06 - INFO - __main__ -   file label: 1
04/23/2024 00:52:06 - INFO - __main__ -   number of functions: 7
04/23/2024 00:52:06 - INFO - __main__ -   function labels: [0, 0, 0, 0, 0, 0, 1]
04/23/2024 00:52:06 - INFO - __main__ -   first function tokens: ['<s>', 'names', 'pace', '_HP', 'HP', '_{', 'using', '_folly', '::', 'IO', 'B', 'uf', ';', 'using', '_folly', '::', 'IO', 'B', 'uf', 'Queue', ';', 'using', '_folly', '::', 'io', '::', 'C', 'ursor', ';', 'const', '_void', '_if', '_(', 'm', '_', 'first', 'Body', ')', '_{', '_CH', 'ECK', '(', 'm', '_', 'cur', 'r', 'Body', ');', '_size', '_=', '_m', '_', 'cur', 'r', 'Body', '->', 'length', '();', '_return', '_m', '_', 'cur', 'r', 'Body', '->', 'data', '();', '_}', '_return', '_get', 'More', 'Post', 'Data', '(', 'size', ');', '}', '</s>']
04/23/2024 00:52:06 - INFO - __main__ -   first function ids: 0 37815 18851 13064 7331 25522 10928 41660 38304 6454 387 2951 131 10928 41660 38304 6454 387 2951 49382 131 10928 41660 38304 1020 38304 347 47853 131 20836 13842 114 36 119 1215 9502 41024 43 25522 3858 30438 1640 119 1215 17742 338 41024 4397 1836 5457 475 1215 17742 338 41024 46613 16096 47006 671 475 1215 17742 338 41024 46613 23687 47006 35524 671 120 9690 21585 30383 1640 10799 4397 24303 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
04/23/2024 00:52:06 - INFO - __main__ -   *** Example ***
04/23/2024 00:52:06 - INFO - __main__ -   file_id: CWE-119/good_3626_0
04/23/2024 00:52:06 - INFO - __main__ -   file label: 0
04/23/2024 00:52:06 - INFO - __main__ -   number of functions: 3
04/23/2024 00:52:06 - INFO - __main__ -   function labels: [0, 0, 0]
04/23/2024 00:52:06 - INFO - __main__ -   first function tokens: ['<s>', '#', 'if', 'nd', 'ef', '_WIN', '32', '#', 'include', '_<', 'sys', '/', 'types', '.', 'h', '>', '#', 'include', '_<', 'sys', '/', 'socket', '.', 'h', '>', '#', 'include', '_<', 'err', 'no', '.', 'h', '>', '#', 'include', '_<', 'net', 'inet', '/', 'in', '.', 'h', '>', '#', 'include', '_<', 'arp', 'a', '/', 'inet', '.', 'h', '>', '#', 'else', '#', 'include', '_"', 'ins', 'p', 'irc', 'd', '_', 'win', '32', 'wrapper', '.', 'h', '"', '#', 'endif', '</s>']
04/23/2024 00:52:06 - INFO - __main__ -   first function ids: 0 10431 1594 1187 4550 17164 2881 10431 47209 28696 43103 73 41817 4 298 15698 10431 47209 28696 43103 73 48154 4 298 15698 10431 47209 28696 14385 2362 4 298 15698 10431 47209 28696 4135 28430 73 179 4 298 15698 10431 47209 28696 11230 102 73 28430 4 298 15698 10431 44617 10431 47209 22 1344 642 21163 417 1215 5640 2881 49247 4 298 113 10431 49741 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
04/23/2024 00:52:14 - INFO - __main__ -   ***** Running training *****
04/23/2024 00:52:14 - INFO - __main__ -     Num examples = 202
04/23/2024 00:52:14 - INFO - __main__ -     Num Epochs = 20
04/23/2024 00:52:14 - INFO - __main__ -     Batch size = 1
04/23/2024 00:52:14 - INFO - __main__ -     Total train batch size = 8
04/23/2024 00:52:14 - INFO - __main__ -     Gradient Accumulation steps = 8
04/23/2024 00:52:14 - INFO - __main__ -     Total optimization steps = 4040
  0%|          | 0/202 [00:00<?, ?it/s]epoch 0 loss -0.08847:   0%|          | 0/202 [00:07<?, ?it/s]epoch 0 loss -0.08847:   0%|          | 1/202 [00:07<25:11,  7.52s/it]epoch 0 loss -0.0889:   0%|          | 1/202 [00:07<25:11,  7.52s/it] epoch 0 loss -0.0889:   1%|          | 2/202 [00:07<10:54,  3.27s/it]epoch 0 loss -0.08656:   1%|          | 2/202 [00:09<10:54,  3.27s/it]epoch 0 loss -0.08656:   1%|▏         | 3/202 [00:09<07:53,  2.38s/it]epoch 0 loss -0.08742:   1%|▏         | 3/202 [00:09<07:53,  2.38s/it]epoch 0 loss -0.08742:   2%|▏         | 4/202 [00:09<05:05,  1.54s/it]epoch 0 loss -0.08818:   2%|▏         | 4/202 [00:09<05:05,  1.54s/it]epoch 0 loss -0.08818:   2%|▏         | 5/202 [00:09<03:26,  1.05s/it]epoch 0 loss -0.08854:   2%|▏         | 5/202 [00:09<03:26,  1.05s/it]epoch 0 loss -0.08854:   3%|▎         | 6/202 [00:09<02:42,  1.21it/s]epoch 0 loss -0.08718:   3%|▎         | 6/202 [00:10<02:42,  1.21it/s]epoch 0 loss -0.08718:   3%|▎         | 7/202 [00:10<02:45,  1.18it/s]epoch 0 loss -0.08761:   3%|▎         | 7/202 [00:10<02:45,  1.18it/s]epoch 0 loss -0.08761:   4%|▍         | 8/202 [00:10<02:00,  1.61it/s]epoch 0 loss -0.0881:   4%|▍         | 8/202 [00:11<02:00,  1.61it/s] epoch 0 loss -0.0881:   4%|▍         | 9/202 [00:11<01:50,  1.75it/s]epoch 0 loss -0.08847:   4%|▍         | 9/202 [00:11<01:50,  1.75it/s]epoch 0 loss -0.08847:   5%|▍         | 10/202 [00:11<01:27,  2.18it/s]epoch 0 loss -0.0891:   5%|▍         | 10/202 [00:11<01:27,  2.18it/s] epoch 0 loss -0.08893:   5%|▍         | 10/202 [00:11<01:27,  2.18it/s]epoch 0 loss -0.08893:   6%|▌         | 12/202 [00:11<00:52,  3.59it/s]epoch 0 loss -0.08841:   6%|▌         | 12/202 [00:11<00:52,  3.59it/s]epoch 0 loss -0.08841:   6%|▋         | 13/202 [00:11<00:46,  4.10it/s]epoch 0 loss -0.08841:   6%|▋         | 13/202 [00:12<03:05,  1.02it/s]
Traceback (most recent call last):
  File "/home/u200110901/jupyterlab/Defect-detection/MIL_instance_level/code/run.py", line 442, in <module>
    main()
  File "/home/u200110901/jupyterlab/Defect-detection/MIL_instance_level/code/run.py", line 422, in main
    train(args, train_dataset, model, tokenizer)
  File "/home/u200110901/jupyterlab/Defect-detection/MIL_instance_level/code/run.py", line 158, in train
    loss, logits, _ = model(functions_inputs, functions_labels, file_label)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u200110901/jupyterlab/Defect-detection/MIL_instance_level/code/model.py", line 30, in forward
    outputs = self.encoder(input_functions_ids[0], attention_mask=input_functions_ids[0].ne(1))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py", line 835, in forward
    encoder_outputs = self.encoder(
                      ^^^^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py", line 524, in forward
    layer_outputs = layer_module(
                    ^^^^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py", line 413, in forward
    self_attention_outputs = self.attention(
                             ^^^^^^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py", line 340, in forward
    self_outputs = self.self(
                   ^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/u200110901/.conda/envs/codebert/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py", line 236, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 316.00 MiB. GPU 0 has a total capacity of 23.67 GiB of which 214.56 MiB is free. Process 9230 has 23.46 GiB memory in use. Of the allocated memory 18.80 GiB is allocated by PyTorch, and 3.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
